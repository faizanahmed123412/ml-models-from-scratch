# =============================================================================
# Step 0: Imports and Environment Check
# =============================================================================
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, MultiHeadAttention, GlobalAveragePooling1D, Dense, Dropout, LayerNormalization
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import matplotlib.pyplot as plt
import seaborn as sns

print("--- Using TensorFlow Version ---")
print(tf.__version__)


# =============================================================================
# Step 1: Data Preparation
#=============================================================================
print("\n--- Step 1: Preparing Data ---")
reviews = [
    "this movie was amazing and fantastic", "I absolutely loved this film",
    "a truly wonderful and great experience", "the acting was superb",
    "simply the best movie ever", "what a terrible and boring movie",
    "I really hated this film", "a disappointing and awful experience",
    "the plot was predictable and bad", "I would not recommend this to anyone",
    # Added more examples
    "the special effects were stunning",
    "the characters were not believable",
    "an enjoyable movie from start to finish",
    "the pacing was too slow",
    "highly recommended for sci-fi fans",
    "the ending was very weak",
    "a masterpiece of modern cinema",
    "I fell asleep halfway through",
    "this film exceeded all my expectations",
    "the dialogue was cringeworthy"
]
labels = np.array([1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
                   # Added labels for the new examples
                   1, 0, 1, 0, 1, 0, 1, 0, 1, 0])

# Tokenize the text to convert words to integers
tokenizer = Tokenizer()
tokenizer.fit_on_texts(reviews)
sequences = tokenizer.texts_to_sequences(reviews)

# Pad sequences so they are all the same length
max_length = max(len(seq) for seq in sequences)
X = pad_sequences(sequences, maxlen=max_length, padding='post')
y = labels
print("Data preprocessing complete.")


# =============================================================================
# Step 2: Build the Transformer Model with the Functional API
# =============================================================================
print("\n--- Step 2: Building the Transformer Model ---")
vocab_size = len(tokenizer.word_index) + 1
embedding_dim = 32  # Increased embedding dimension for better representation
num_heads = 4       # Number of attention heads
ff_dim = 32         # Hidden layer size in the feed-forward network

# Define the input layer
inputs = Input(shape=(max_length,))

# 1. Embedding Layer
embedding_layer = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(inputs)

# 2. Transformer Block (Self-Attention)
# Create the attention layer first.
attention_layer = MultiHeadAttention(num_heads=num_heads, key_dim=embedding_dim)

# Now, call the layer and ask for attention scores here.
# CORRECTED CODE: The argument is 'return_attention_scores', not 'output_attention_scores'.
attention_output, attention_scores = attention_layer(
    query=embedding_layer,
    value=embedding_layer,
    return_attention_scores=True # Ask for scores during the call, not creation.
)

# Add & Norm step
add_norm_1 = LayerNormalization(epsilon=1e-6)(embedding_layer + attention_output)

# 3. Pooling Layer to summarize the sentence
pooling_layer = GlobalAveragePooling1D()(add_norm_1)

# 4. Final Classifier
dense_layer = Dense(ff_dim, activation='relu')(pooling_layer)
dropout_layer = Dropout(0.1)(dense_layer)
outputs = Dense(1, activation='sigmoid')(dropout_layer)

# Create the final model
model = Model(inputs=inputs, outputs=outputs)


# =============================================================================
# Step 3: Compile and Train the Model
# =============================================================================
model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

print("\n--- Model Summary ---")
model.summary()

print("\n--- Step 3: Starting Model Training ---")
history = model.fit(X, y, epochs=50, verbose=1)
print("--- Model Training Complete ---")


# =============================================================================
# Step 4: Create the Visualization Model and Function
# =============================================================================
print("\n--- Step 4: Preparing for Visualization ---")
# This new model shares the same layers and weights as our trained model,
# but its output is the attention scores, not the final prediction.
# visualization_model = Model(inputs=inputs, outputs=attention_scores) # This model is no longer needed for prediction

def predict_sentiment(text):
    """
    Takes a text string, preprocesses it, and predicts the sentiment
    (positive or negative) using the trained Transformer model.
    Returns the predicted sentiment string and the probability.
    """
    # Preprocess the input text
    sequence = tokenizer.texts_to_sequences([text])
    padded_sequence = pad_sequences(sequence, maxlen=max_length, padding='post')

    # Get the prediction from the trained model
    prediction = model.predict(padded_sequence)

    # Determine sentiment based on the prediction probability
    sentiment = "Positive" if prediction[0][0] > 0.5 else "Negative"

    return sentiment, prediction[0][0]


# =============================================================================
# Step 5: Test and Visualize
# =============================================================================
print("\n--- Step 5: Visualizing Self-Attention in Action ---")

# Test with a positive sentence
sentiment, probability = predict_sentiment("this film was absolutely great")
print(f"Review: 'this film was absolutely great'")
print(f"Predicted Sentiment: {sentiment} (Probability: {probability:.2f})")

# Test with a negative sentence
sentiment, probability = predict_sentiment("the plot was awful and boring")
print(f"Review: 'the plot was awful and boring'")
print(f"Predicted Sentiment: {sentiment} (Probability: {probability:.2f})")

sentiment, probability = predict_sentiment("this film was not very good")
print(f"Review: 'this film was not very good'")
print(f"Predicted Sentiment: {sentiment} (Probability: {probability:.2f})")

# Add new examples
sentiment, probability = predict_sentiment("I really enjoyed this movie")
print(f"Review: 'I really enjoyed this movie'")
print(f"Predicted Sentiment: {sentiment} (Probability: {probability:.2f})")

sentiment, probability = predict_sentiment("the acting was terrible")
print(f"Review: 'the acting was terrible'")
print(f"Predicted Sentiment: {sentiment} (Probability: {probability:.2f})")

# Add more diverse examples
sentiment, probability = predict_sentiment("the movie was okay, not great but not bad either")
print(f"Review: 'the movie was okay, not great but not bad either'")
print(f"Predicted Sentiment: {sentiment} (Probability: {probability:.2f})")

sentiment, probability = predict_sentiment("I was completely blown away by this film")
print(f"Review: 'I was completely blown away by this film'")
print(f"Predicted Sentiment: {sentiment} (Probability: {probability:.2f})")

sentiment, probability = predict_sentiment("a complete waste of time")
print(f"Review: 'a complete waste of time'")
print(f"Predicted Sentiment: {sentiment} (Probability: {probability:.2f})")

sentiment, probability = predict_sentiment("this is the best movie I have ever seen")
print(f"Review: 'this is the best movie I have ever seen'")
print(f"Predicted Sentiment: {sentiment} (Probability: {probability:.2f})")

sentiment, probability = predict_sentiment("I did not like this movie at all")
print(f"Review: 'I did not like this movie at all'")
print(f"Predicted Sentiment: {sentiment} (Probability: {probability:.2f})")